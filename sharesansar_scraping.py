# -*- coding: utf-8 -*-
"""ShareSansar_scraping.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bYHJNsN9yTS_i7o4znxueYmMSw3Ws5n7

Web Scraping Project by Manjil Karki and Anushil Timsina.

Scraping Share Prices according to companies from 'Share Sansar.'

Website - 'https://www.sharesansar.com/'

Strictly for Educational Purpose. Do not misuse.
"""

# Install these at first

# ! pip install selenium
# !apt-get update 
# !apt install chromium-chromedriver

# Importing

from selenium import webdriver
from bs4 import BeautifulSoup
import time
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import Select
import csv

# Setting up Browser

chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')
driver = webdriver.Chrome('chromedriver',options = chrome_options)

# Getting the response

base_url = "https://www.sharesansar.com/company-list"
driver.get(base_url)
dr1 = driver

select = Select(dr1.find_element(By.NAME,'sector'))
#(select.options)

# Find the Sectors

opts = []
i = 0

for opt in select.options:
  opts.append(opt.text)
  print(opt.text+'    ->   ',i)
  i = i + 1

sel = int(input('Select the Sector whose data you need:'))
select.select_by_index(sel)
dr1.find_element(By.ID,'btn_listed_submit').click()
print('\nYou Selected:',opts[sel])

# Find the total organizations in the selected sector

time.sleep(5)
organizations = {}
while True:
  soup = BeautifulSoup(dr1.page_source)
  all_banks = soup.find('table',attrs={'id':'myTable'})
  all_data = all_banks.find('tbody').find_all('tr')      


  for row in all_data:
    table_data = row.find_all('td')
    key = f"{table_data[2].text}({table_data[1].text})"
    val = table_data[1].find('a')['href']      

    organizations[key] = val

  try:
    elem = dr1.find_element(By.ID, 'myTable_next')
    check = elem.get_attribute('class').split('')[-1]
    if check == 'disabled':
      break
    dr1.find_element(By.ID,'myTable_next').click()
    time.sleep(10)
  except:
    break

  print(f'we have {len(organizations)} organizations')
  time.sleep(5)

# Select the req organiation

print(f"which {opts[sel]} organizations do you like to scrape:")
i = 0
names = []
for key in organizations:
  print(key + '   ->   ',i)
  names.append(key)
  i = i+1

sel = int(input("Enter reference number:"))
print('\nYou selected:',names[sel])
keys = names[sel]
#print(key)

# Scrape the data from the selected organization

dr1.get(organizations[key])

dr1.find_element(By.ID,'btn_cpricehistory').click()

select = Select(dr1.find_element(By.NAME,'myTableCPriceHistory_length'))
select.select_by_index(2)

run_once = 1
while True:

  time.sleep(2)
  soup = BeautifulSoup(dr1.page_source)
  table = soup.find('table',attrs={'id':'myTableCPriceHistory'})

  if run_once == 1:
    headings = table.find('thead').find_all('th')
    headings = [heading.text for heading in headings]
    with open(key+'.csv','w',newline='') as output_file:
      dict_writer = csv.DictWriter(output_file,fieldnames=headings)
      dict_writer.writeheader()
    run_once = 0


  table = table.find('tbody').find_all('tr')

  table_data = []
  for row in table:
    data = row.find_all('td')
    data = [d.text for d in data]
    table_data.append(dict(zip(headings,data)))

    print(f'Found {len(table_data)} rows in this page')

    with open(key+".csv", 'a', newline='') as output_file:
      dict_writer = csv.DictWriter(output_file, fieldnames = headings)   
      dict_writer.writerows(table_data)

  try:
    dr1.find_element(By.ID, "myTableCPriceHistory_next").click()
    print("scraping next page")
  except:
    print("your work is done!!")
    break

